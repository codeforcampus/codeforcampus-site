<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>9. Dimensionality Reduction - Machine Learning Course</title>
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background-color: #f8fafc;
      color: #334155;
      line-height: 1.6;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    h1 {
      font-size: 2.2rem;
      color: #1e40af;
      margin-bottom: 20px;
    }

    h2 {
      font-size: 1.6rem;
      color: #1d4ed8;
      margin-top: 40px;
    }

    h3 {
      font-size: 1.3rem;
      color: #0f172a;
      margin-top: 30px;
    }

    p {
      font-size: 1rem;
    }

    ul {
      margin-top: 10px;
      margin-bottom: 20px;
      padding-left: 20px;
    }

    .highlight, .key-points {
      background-color: #e0f2fe;
      padding: 12px 15px;
      border-left: 5px solid #0284c7;
      margin: 20px 0;
      border-radius: 6px;
    }

    code {
      background-color: #f1f5f9;
      padding: 2px 6px;
      border-radius: 4px;
      font-family: monospace;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 20px;
      background-color: #ffffff;
      border: 1px solid #e2e8f0;
    }

    th, td {
      border: 1px solid #e2e8f0;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #e0f2fe;
      color: #0369a1;
    }

    .toc {
      background-color: #f1f5f9;
      border: 1px solid #cbd5e1;
      padding: 15px 20px;
      border-radius: 6px;
      margin-bottom: 30px;
    }

    .toc ul {
      list-style: none;
      padding-left: 0;
    }

    .toc li {
      margin-bottom: 8px;
    }

    .toc a {
      color: #1d4ed8;
      text-decoration: none;
    }

    .toc a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <div class="container">
    <h1>9. Dimensionality Reduction</h1>
    <p>Dimensionality reduction helps simplify datasets by reducing the number of input features, making models more efficient and easier to visualize.</p>

    <!-- Table of Contents -->
    <div class="toc">
      <strong>ðŸ“š Table of Contents</strong>
      <ul>
        <li><a href="#pca-technique">9.1 PCA</a></li>
        <li><a href="#pc-components">9.1.1 PC1 & PC2</a></li>
        <li><a href="#projection">9.1.2 Projection</a></li>
        <li><a href="#visualization">9.1.3 Visualization</a></li>
      </ul>
    </div>

    <h2 id="pca-technique">9.1 Principal Component Analysis (PCA)</h2>
    <p>PCA is a statistical method that transforms the original features into a new set of orthogonal axes (principal components) capturing maximum variance.</p>
    <div class="highlight">
      <code>PC = X * W</code> where X is the data matrix, and W contains the eigenvectors
    </div>
    <div class="key-points">
      <strong>Key Points:</strong>
      <ul>
        <li>Unsupervised technique used for preprocessing and visualization</li>
        <li>Maximizes variance in lower-dimensional space</li>
        <li>Orthogonal components reduce redundancy</li>
      </ul>
    </div>

    <h3 id="pc-components">9.1.1 PC1 & PC2</h3>
    <p>Principal components are ranked by variance explained. PC1 captures the most variance, followed by PC2, and so on.</p>
    <ul>
      <li><strong>PC1</strong>: The direction of maximum variance</li>
      <li><strong>PC2</strong>: Orthogonal to PC1 and captures remaining variance</li>
    </ul>
    <div class="highlight">
      <code>Explained Variance Ratio = (Î»áµ¢ / Î£Î»)</code>
    </div>
    <div class="key-points">
      <strong>Key Points:</strong>
      <ul>
        <li>PC1 and PC2 often used for 2D visualizations</li>
        <li>Each PC is a linear combination of original features</li>
        <li>Helps identify dominant patterns in data</li>
      </ul>
    </div>

    <h3 id="projection">9.1.2 Projection</h3>
    <p>Data points are projected onto the lower-dimensional space formed by selected principal components.</p>
    <div class="highlight">
      <code>Projected_X = X Â· W<sub>k</sub></code> where W<sub>k</sub> are top-k eigenvectors
    </div>
    <p>This projection preserves as much variance as possible while reducing dimensionality.</p>
    <div class="key-points">
      <strong>Key Points:</strong>
      <ul>
        <li>Projection helps visualize high-dimensional data in 2D or 3D</li>
        <li>Reduces overfitting by eliminating noise</li>
        <li>Improves model training time and performance</li>
      </ul>
    </div>

    <h3 id="visualization">9.1.3 Visualization</h3>
    <p>PCA is commonly used to plot data in 2D or 3D. Useful for clustering analysis and detecting outliers.</p>
    <ul>
      <li>Each point is a data sample projected onto PC1 and PC2</li>
      <li>Coloring based on class helps visualize separability</li>
    </ul>
    <div class="key-points">
      <strong>Key Points:</strong>
      <ul>
        <li>Enables exploratory data analysis (EDA)</li>
        <li>Good for checking clustering tendencies visually</li>
        <li>Works best when top 2 PCs capture enough variance</li>
      </ul>
    </div>

    <!-- Summary Table -->
    <table>
      <tr>
        <th>Concept</th>
        <th>Description</th>
      </tr>
      <tr>
        <td>PCA</td>
        <td>Transforms data using orthogonal axes maximizing variance</td>
      </tr>
      <tr>
        <td>PC1, PC2</td>
        <td>Top principal components ranked by variance captured</td>
      </tr>
      <tr>
        <td>Projection</td>
        <td>Maps high-dimensional data onto reduced subspace</td>
      </tr>
      <tr>
        <td>Visualization</td>
        <td>Plots data on top PCs to explore patterns</td>
      </tr>
    </table>
  </div>

</body>
</html>
