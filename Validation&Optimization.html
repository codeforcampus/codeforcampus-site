<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Section 5 - Validation & Optimization</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      margin: 0;
      padding: 0;
      background: #f8fafc;
      color: #111827;
    }

    .main {
      max-width: 900px;
      margin: 40px auto;
      padding: 30px;
      background: #ffffff;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.07);
    }

    h1 {
      font-size: 32px;
      color: #1e3a8a;
      margin-bottom: 20px;
      border-bottom: 2px solid #3b82f6;
      padding-bottom: 10px;
    }

    h2 {
      font-size: 24px;
      margin-top: 40px;
      color: #2563eb;
      border-left: 6px solid #3b82f6;
      padding-left: 12px;
    }

    p {
      font-size: 16px;
      line-height: 1.7;
      margin: 15px 0;
    }

    ul {
      margin: 10px 0 20px 20px;
    }

    ul li {
      font-size: 16px;
      margin-bottom: 8px;
    }

    .highlight {
      background-color: #eff6ff;
      border-left: 5px solid #3b82f6;
      padding: 12px 16px;
      margin: 20px 0;
      border-radius: 6px;
      color: #0f172a;
    }

    .highlight ul {
      list-style-type: disc;
      padding-left: 20px;
      margin: 10px 0 0 0;
    }

    strong {
      color: #1e40af;
    }

    @media (max-width: 768px) {
      .main {
        margin: 20px;
        padding: 20px;
      }

      h1 {
        font-size: 26px;
      }

      h2 {
        font-size: 20px;
      }
    }
  </style>
</head>
<body>

  <div class="main">
    <h1>5. Validation & Optimization</h1>

    <!-- 5.1 Validation Set -->
    <section id="valset">
      <h2>5.1 Validation Set</h2>
      <p><strong>Definition:</strong> A validation set is a separate portion of the data used during model training to evaluate performance and tune hyperparameters. It helps prevent overfitting and ensures the model generalizes well.</p>
      <p><strong>Explanation:</strong> While the training set teaches the model, the validation set acts like a mock test. It's not used to train, but to evaluate progress during tuning.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul>
        <li><strong>1. Face Recognition System:</strong> A validation set with different lighting conditions checks if the system works beyond training images.</li>
        <li><strong>2. Language Translation App:</strong> A subset of multilingual sentences is used to validate translation quality before final deployment.</li>
      </ul>
      <div class="highlight">
        ðŸ’¡ <strong>Key Points:</strong>
        <ul>
          <li>Used during training, but not for weight updates.</li>
          <li>Helps in early stopping and hyperparameter tuning.</li>
          <li>Essential for detecting overfitting.</li>
        </ul>
      </div>
      <div class="highlight">
        ðŸ“Š <strong>Facts:</strong><br>
        - Not the same as a test set.<br>
        - Often used in k-fold cross-validation.<br>
        - Should represent unseen scenarios.
      </div>
    </section>

    <!-- 5.2 Learning Rate -->
    <section id="learningrate">
      <h2>5.2 Learning Rate</h2>
      <p><strong>Definition:</strong> Learning rate is a hyperparameter that controls how much the model updates weights during training in response to error.</p>
      <p><strong>Explanation:</strong> A small learning rate leads to slow convergence, while a large one might skip optimal solutions or diverge entirely.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul>
        <li><strong>1. Handwriting Digit Recognition:</strong> Tuning the learning rate improves digit classification in MNIST dataset.</li>
        <li><strong>2. Stock Price Prediction:</strong> A small learning rate stabilizes prediction when dealing with volatile markets.</li>
      </ul>
      <div class="highlight">
        ðŸ’¡ <strong>Key Points:</strong>
        <ul>
          <li>Too high = unstable; too low = slow training.</li>
          <li>Learning rate schedules (like decay) help optimization.</li>
          <li>Adaptive optimizers (e.g., Adam) adjust learning rate dynamically.</li>
        </ul>
      </div>
      <div class="highlight">
        ðŸ“Š <strong>Facts:</strong><br>
        - Typical values: 0.01, 0.001, etc.<br>
        - Can be static or use decay techniques.<br>
        - Crucial for model convergence.
      </div>
    </section>

    <!-- 5.3 Feature Scaling -->
    <section id="scaling">
      <h2>5.3 Feature Scaling</h2>
      <p><strong>Definition:</strong> Feature scaling is a preprocessing technique that standardizes or normalizes input features to be on the same scale.</p>
      <p><strong>Explanation:</strong> Algorithms like gradient descent or KNN are sensitive to the scale of data. Scaling improves training speed and accuracy.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul>
        <li><strong>1. Medical Diagnosis:</strong> Normalize patient data like height (in cm) and weight (in kg) to ensure fairness in analysis.</li>
        <li><strong>2. E-commerce Recommendation:</strong> Scale price, rating, and reviews to prevent bias in recommendation engines.</li>
      </ul>
      <div class="highlight">
        ðŸ’¡ <strong>Key Points:</strong>
        <ul>
          <li>Popular methods: Min-Max Scaling, Standardization (Z-score).</li>
          <li>Essential for distance-based models like KNN, SVM.</li>
          <li>Improves convergence speed of gradient descent.</li>
        </ul>
      </div>
      <div class="highlight">
        ðŸ“Š <strong>Facts:</strong><br>
        - Raw features may mislead models.<br>
        - Scaling is part of preprocessing pipeline.<br>
        - Applied only to numerical features.
      </div>
    </section>

    <!-- 5.4 Pruning -->
    <section id="pruning">
      <h2>5.4 Pruning</h2>
      <p><strong>Definition:</strong> Pruning is the process of removing parts of a model (like branches in a decision tree) that contribute little to accuracy.</p>
      <p><strong>Explanation:</strong> By trimming unimportant sections, we reduce complexity, prevent overfitting, and improve generalization.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul>
        <li><strong>1. Decision Trees in Loan Approvals:</strong> Prune redundant branches to prevent overfitting to rare customer cases.</li>
        <li><strong>2. Neural Network Compression:</strong> Remove neurons with near-zero weights to optimize memory in mobile apps.</li>
      </ul>
      <div class="highlight">
        ðŸ’¡ <strong>Key Points:</strong>
        <ul>
          <li>Helps control model depth and complexity.</li>
          <li>Reduces training time and improves inference speed.</li>
          <li>Common in decision trees and deep networks.</li>
        </ul>
      </div>
      <div class="highlight">
        ðŸ“Š <strong>Facts:</strong><br>
        - Types: Pre-pruning (early stopping) & Post-pruning (after full training).<br>
        - Works well with tree-based models.<br>
        - Makes models more interpretable.
      </div>
    </section>

    <!-- 5.5 Hyperparameter Tuning -->
    <section id="hyper">
      <h2>5.5 Hyperparameter Tuning</h2>
      <p><strong>Definition:</strong> Hyperparameter tuning is the process of selecting the best set of model settings (like learning rate, depth) to improve performance.</p>
      <p><strong>Explanation:</strong> Unlike model parameters (learned), hyperparameters are manually set. Tuning helps identify the optimal configuration through experiments.</p>
      <p><strong>Real-Life Examples:</strong></p>
      <ul>
        <li><strong>1. Random Forest Model:</strong> Tuning the number of trees and max depth improves accuracy on a housing price dataset.</li>
        <li><strong>2. Neural Network:</strong> Tuning batch size, learning rate, and layers improves performance on image classification tasks.</li>
      </ul>
      <div class="highlight">
        ðŸ’¡ <strong>Key Points:</strong>
        <ul>
          <li>Common methods: Grid Search, Random Search, Bayesian Optimization.</li>
          <li>Use cross-validation for reliable results.</li>
          <li>Automation tools like Optuna and KerasTuner simplify the process.</li>
        </ul>
      </div>
      <div class="highlight">
        ðŸ“Š <strong>Facts:</strong><br>
        - Hyperparameters are set before training.<br>
        - Impact overall model performance.<br>
        - Tuning can be time-consuming but is crucial.
      </div>
    </section>
  </div>

</body>
</html>
